# @package _global_

motion_name: RL_Avatar_Atk_2xCombo01_Motion

algo:
  config:

    dataset_file: output/recordings/mimic_combo/dataset.hdf5

    attribs_to_import:
      - obs
      - mimic_target_poses
      - actions
      - rewards
      - dones

    models:
      terrain_models:
        mlp: null
        transformer: null
      mimic_target_pose_model:
        _target_: phys_anim.agents.models.common.Flatten
        config:
          normalize_obs: ${algo.config.normalize_obs}
          obs_clamp_value: ${algo.config.obs_clamp_value}
        num_in: ${eval:${env.config.mimic_target_pose.num_future_steps}*${env.config.mimic_target_pose.num_obs_per_target_pose}}
        num_out: ${.num_in}
    # Setup discriminator structure
    actor:
      _target_: phys_anim.agents.models.actor.ActorFixedSigma
      _recursive_: false
      config:
        initializer: default
        mu_model:
          _target_: phys_anim.agents.models.mlp.MultiHeadedMLP
          _recursive_: false
          config:
            initializer: ${algo.config.actor.config.initializer}
            units: [512, 512, 512, 512, 512, 512]
            activation: ${algo.config.actor.config.activation}
            normalize_obs: ${algo.config.normalize_obs}
            obs_clamp_value: ${algo.config.obs_clamp_value}
            use_layer_norm: ${algo.config.actor.config.use_layer_norm}
            terrain_model: ${algo.config.models.terrain_models.mlp}
            extra_inputs:
              mimic_target_poses: ${algo.config.models.mimic_target_pose_model}
            latent_dim: ${algo.config.infomax_parameters.latent_dim}
        init_logstd: -2.9
        learnable_sigma: false
        sigma_schedule: null
        use_layer_norm: false
        activation: relu

    critic_s:
      _target_: phys_anim.agents.models.critic.CriticMLP
      _recursive_: False
      config:
        initializer: default
        units: [1024, 1024, 1024]
        activation: relu
        normalize_obs: ${algo.config.normalize_obs}
        obs_clamp_value: ${algo.config.obs_clamp_value}
        use_layer_norm: False
        extra_inputs:
          mimic_target_poses: ${algo.config.models.mimic_target_pose_model}

    critic_sa:
      _target_: phys_anim.agents.models.critic.CriticMLP
      _recursive_: False
      config:
        initializer: default
        units: [ 1024, 512 ]
        activation: relu
        normalize_obs: ${algo.config.normalize_obs}
        obs_clamp_value: ${algo.config.obs_clamp_value}
        use_layer_norm: False
        extra_inputs:
          actions:
            _target_: phys_anim.agents.models.common.Flatten
            config:
              normalize_obs: False
              obs_clamp_value: ${algo.config.obs_clamp_value}
            num_in: ${robot.number_of_actions}
            num_out: ${.num_in}
          mimic_target_poses: ${algo.config.models.mimic_target_pose_model}

    actor_optimizer:
      _target_: torch.optim.Adam
      lr: 2e-5
      betas: [0.9, 0.999]

    critic_optimizer:
      _target_: torch.optim.Adam
      lr: 1e-4
      betas: [0.9, 0.999]

    actor_lr_scheduler: null
    critic_lr_scheduler: null

    # ASE parameters
    infomax_parameters:
      latent_dim: [ 64 ]
      latent_types: [ hypersphere ]

      mi_reward_w: [ 0.5 ]
      mi_hypersphere_reward_shift: True

      mi_enc_weight_decay: 0
      mi_enc_grad_penalty: 0

      diversity_tar: 1.
      diversity_bonus: 0.01

      random_latents: True
      latent_steps_min: 1
      latent_steps_max: 150

    #IQL parameters
    expectile: 0.7
    alpha: 0.005
    batch_size: 16384
    max_epochs: 10000
    normalize_obs: True
    obs_clamp_value: null
    discriminator_reward_w: 2.0
    discriminator_grad_penalty: 5
    discriminator_weight_decay: 0.0001
    discriminator_logit_weight_decay: 0.01
    discount: 0.99
    beta: 3 # aka temperatue

    eval_callbacks: null

    extra_inputs: null

    num_obs_enc_steps: 60

env:
  config:
    # Simulation params
    ## Observations
    humanoid_obs:
      use_max_coords_obs: True
      local_root_obs: True
      root_height_obs: True

    mimic_target_pose:
      enabled: True
      type: max-coords-future-rel
      with_time: False
      num_future_steps: 5
      num_obs_per_target_pose: ${.base_num_obs_per_target_pose}
      base_num_obs_per_target_pose: ${eval:${.num_obs_per_joint}*${robot.num_bodies}}
      num_obs_per_joint: 18

    discriminator_obs_historical_steps: 10
    disable_discriminator: False
    discriminator_obs_size_per_step: ${eval:13+${robot.dof_obs_size}+${robot.number_of_actions}+3*${robot.num_key_bodies}}

    ## Motion-related params
    ### Respawn related params
    state_init: Random